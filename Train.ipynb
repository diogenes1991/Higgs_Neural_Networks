{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Importing the data from the ROOT files\n",
    "    All root data files can be obtained through the open data \n",
    "    program from the ATLAS collaboration. \n",
    "    They can be found here:\n",
    "        http://opendata.atlas.cern/samples-13tev/\n",
    "    All data files are in root format and require \n",
    "    the installation of ROOT's python interface. \n",
    "    ROOT is a CERN C++ Library which can be found here:\n",
    "        https://root.cern.ch/\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import ROOT\n",
    "from ROOT import TMath\n",
    "from DataLoader import LoadDataAndProcess\n",
    "\n",
    "ttH = ROOT.TFile.Open(\"mc_341081.ttH125_gamgam.GamGam.root\")\n",
    "ggH = ROOT.TFile.Open(\"mc_343981.ggH125_gamgam.GamGam.root\")\n",
    "WWH = ROOT.TFile.Open(\"mc_345041.VBFH125_gamgam.GamGam.root\")\n",
    "data1 = ROOT.TFile.Open(\"data_A.GamGam.root\")\n",
    "data2 = ROOT.TFile.Open(\"data_B.GamGam.root\")\n",
    "data3 = ROOT.TFile.Open(\"data_C.GamGam.root\")\n",
    "data4 = ROOT.TFile.Open(\"data_D.GamGam.root\")\n",
    "\n",
    "# Channel Dictionary\n",
    "Channels = {\"ttH\":ttH.Get(\"mini\"),\n",
    "            \"ggH\":ggH.Get(\"mini\"),\n",
    "            \"WWH\":WWH.Get(\"mini\"), \n",
    "            'data1':data1.Get('mini'), \n",
    "            'data2':data2.Get('mini'),\n",
    "            'data3':data3.Get('mini'),\n",
    "            'data4':data4.Get('mini')\n",
    "        }\n",
    "\n",
    "# Loading the data into the dicrtionaries\n",
    "for channel in Channels:\n",
    "    print(\"Channel:\",channel,\"has\",Channels[channel].GetEntries(),\"entries\")\n",
    "\n",
    "# This outputs refer to the targets for the Neural Network\n",
    "# Initially all signal channels are differentiated from each other\n",
    "OutputMap = {'ttH':[1,0,0,0],'ggH':[0,1,0,0],'WWH':[0,0,1,0], \n",
    "             'data1':[0,0,0,1], 'data2':[0,0,0,1], 'data3':[0,0,0,1], 'data4':[0,0,0,1]}\n",
    "\n",
    "\n",
    "# Sort Photons by Energy\n",
    "def SortAndFlatten(Particles, SortFunction):\n",
    "    datapoint = []\n",
    "    Particles.sort(key = SortFunction)\n",
    "    for particle in Particles:\n",
    "        datapoint.append(particle.E())\n",
    "        datapoint.append(particle.Px())\n",
    "        datapoint.append(particle.Py())\n",
    "        datapoint.append(particle.Pz())\n",
    "    return datapoint\n",
    "\n",
    "# Compute invariant mass and sort by energy\n",
    "def SortFlattenAndInvariantMass(Particles, SortFunction):\n",
    "    datapoint = []\n",
    "    Particles.sort(key = SortFunction)\n",
    "    datapoint.append(np.sqrt(2*(Particles[0].E()**2-Particles[0].Px()**2-Particles[0].Py()**2-Particles[0].Pz()**2)))\n",
    "    for particle in Particles:\n",
    "        datapoint.append(particle.Px())\n",
    "        datapoint.append(particle.Py())\n",
    "        datapoint.append(particle.Pz())\n",
    "    return datapoint\n",
    "\n",
    "'''\n",
    "    Loading both datasets from the ROOT files into \n",
    "    we also apply a sorting to both datasets\n",
    "\n",
    "'''\n",
    "\n",
    "UnprocessedLambda  = lambda Photons : SortAndFlatten( Photons, lambda Photon : -Photon.E() )\n",
    "UnprocessedDataset = LoadDataAndProcess(Channels, OutputMap, UnprocessedLambda, 50000 )\n",
    "\n",
    "PreprocessedLambda  = lambda Photons : SortFlattenAndInvariantMass(Photons, lambda Photon : -Photon.E() )\n",
    "PreprocessedDataset = LoadDataAndProcess(Channels, OutputMap, PreprocessedLambda, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Model\n",
    "\n",
    "UnProcessedModel  = Model(\"Unprocessed Model\" ,\"./Unprocessed\" ,UnprocessedDataset)\n",
    "PreprocessedModel = Model(\"Preprocessed Model\",\"./Preprocessed\",PreprocessedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnProcessedModel.train(0.35,100,True)\n",
    "PreprocessedModel.train(0.35,100,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnProcessedModel.analyze()\n",
    "PreprocessedModel.analyze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1cb35124d7b8b11792493ad9de20638983bfd92e094d4b9a16158140118f37cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
